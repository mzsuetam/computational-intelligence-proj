{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab377954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb014a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Conv -> BN -> ReLU block used throughout the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    The standard U-Net encoder block: Two Conv3x3 blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            ConvBlock(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class RFB_Skip(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-level RFB-based skip pathways (Section 2.2).\n",
    "    It consists of two parallel paths:\n",
    "    1. A stack of standard 3x3 convolutions (approximating a larger kernel).\n",
    "    2. A single dilated convolution.\n",
    "    The outputs are concatenated.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_stack, dilation_rate):\n",
    "        super(RFB_Skip, self).__init__()\n",
    "        \n",
    "        # Path 1: Stack of standard convolutions (Green blocks in diagram)\n",
    "        # To emulate large kernels like 7x7, we stack 3x3 convs.\n",
    "        stack_layers = []\n",
    "        for _ in range(num_stack):\n",
    "            # 1x1 convs are used in the bottom level as per text, 3x3 elsewhere\n",
    "            k = 3 if num_stack > 1 else 1 \n",
    "            p = 1 if num_stack > 1 else 0\n",
    "            stack_layers.append(ConvBlock(in_channels, in_channels, kernel_size=k, padding=p))\n",
    "        self.stack_path = nn.Sequential(*stack_layers)\n",
    "\n",
    "        # Path 2: Dilated Convolution (Yellow blocks in diagram)\n",
    "        # Note: Section 2.2 mentions matching receptive fields.\n",
    "        # For the bottom level (dilation 1), it acts as a standard conv.\n",
    "        self.dilated_path = ConvBlock(in_channels, in_channels, \n",
    "                                      kernel_size=3, \n",
    "                                      padding=dilation_rate, # Padding must equal dilation to keep size\n",
    "                                      dilation=dilation_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_stack = self.stack_path(x)\n",
    "        out_dilated = self.dilated_path(x)\n",
    "        # Concatenate features from both paths\n",
    "        return torch.cat([out_stack, out_dilated], dim=1)\n",
    "\n",
    "class CSE_UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1):\n",
    "        super(CSE_UNet, self).__init__()\n",
    "        \n",
    "        filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "        # --- ENCODER (Dual-Path) ---\n",
    "        \n",
    "        # Level 1\n",
    "        self.enc1_main = DoubleConv(in_channels, filters[0])\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        # Aux Path 1: Conv7x7 stride 2 (Section 2.3)\n",
    "        self.enc1_aux = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, filters[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(filters[0])\n",
    "        )\n",
    "\n",
    "        # Level 2\n",
    "        self.enc2_main = DoubleConv(filters[0], filters[1])\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        # Aux Path 2: Conv5x5 stride 2\n",
    "        self.enc2_aux = nn.Sequential(\n",
    "            nn.Conv2d(filters[0], filters[1], kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(filters[1])\n",
    "        )\n",
    "\n",
    "        # Level 3\n",
    "        self.enc3_main = DoubleConv(filters[1], filters[2])\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        # Aux Path 3: Conv3x3 stride 2\n",
    "        self.enc3_aux = nn.Sequential(\n",
    "            nn.Conv2d(filters[1], filters[2], kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(filters[2])\n",
    "        )\n",
    "\n",
    "        # Level 4\n",
    "        self.enc4_main = DoubleConv(filters[2], filters[3])\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        # Aux Path 4: Conv2x2 stride 2\n",
    "        # Note: 2x2 kernel with stride 2 and padding 0 halves the dimension perfectly\n",
    "        self.enc4_aux = nn.Sequential(\n",
    "            nn.Conv2d(filters[2], filters[3], kernel_size=2, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(filters[3])\n",
    "        )\n",
    "\n",
    "        # Bridge\n",
    "        self.bridge = DoubleConv(filters[3], filters[4])\n",
    "\n",
    "        # --- RFB SKIP PATHWAYS ---\n",
    "        \n",
    "        # According to Section 2.2 and Diagram\n",
    "        # Top (Level 1): Stack of 3, Dilation 7\n",
    "        self.rfb1 = RFB_Skip(filters[0], num_stack=3, dilation_rate=7)\n",
    "        # Level 2: Stack of 2, Dilation 5\n",
    "        self.rfb2 = RFB_Skip(filters[1], num_stack=2, dilation_rate=5)\n",
    "        # Level 3: Stack of 1 (3x3), Dilation 3\n",
    "        self.rfb3 = RFB_Skip(filters[2], num_stack=1, dilation_rate=3)\n",
    "        # Level 4: Stack of 1 (1x1), Dilation 1\n",
    "        # Text says: \"one convolution layer with 1x1... and one dilated... dilation rate of 1\"\n",
    "        # We pass num_stack=1, but internal logic handles the 1x1 kernel switch for the bottom layer\n",
    "        self.rfb4 = RFB_Skip(filters[3], num_stack=1, dilation_rate=1) \n",
    "        \n",
    "        # Note on RFB4: The stack logic in `RFB_Skip` uses 3x3 by default. \n",
    "        # We need to manually override or create a specific block if strict adherence to \"1x1\" \n",
    "        # for the stack path is required. I added logic in RFB_Skip to handle this.\n",
    "\n",
    "        # --- DECODER ---\n",
    "        \n",
    "        # Since RFB concatenates output, skip channels are doubled\n",
    "        \n",
    "        # Decoder 4\n",
    "        self.up4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up_conv4 = nn.Conv2d(filters[4], filters[3], kernel_size=1) # Conv1x1 after upsample\n",
    "        self.dec4 = DoubleConv(filters[3] + (filters[3] * 2), filters[3]) # Input = Prev + Skip(RFB x2)\n",
    "\n",
    "        # Decoder 3\n",
    "        self.up3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up_conv3 = nn.Conv2d(filters[3], filters[2], kernel_size=1)\n",
    "        self.dec3 = DoubleConv(filters[2] + (filters[2] * 2), filters[2])\n",
    "\n",
    "        # Decoder 2\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up_conv2 = nn.Conv2d(filters[2], filters[1], kernel_size=1)\n",
    "        self.dec2 = DoubleConv(filters[1] + (filters[1] * 2), filters[1])\n",
    "\n",
    "        # Decoder 1\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up_conv1 = nn.Conv2d(filters[1], filters[0], kernel_size=1)\n",
    "        self.dec1 = DoubleConv(filters[0] + (filters[0] * 2), filters[0])\n",
    "\n",
    "        # Final Output\n",
    "        self.final_conv = nn.Conv2d(filters[0], num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- ENCODER + DUAL PATH FUSION ---\n",
    "        \n",
    "        # Stage 1\n",
    "        x1_main = self.enc1_main(x)\n",
    "        x1_pool = self.pool1(x1_main)\n",
    "        x1_aux = self.enc1_aux(x)\n",
    "        # Fusion: Main (Pooled) + Aux\n",
    "        x1_fused = x1_pool + x1_aux \n",
    "        # Apply ReLU after addition (Standard ResNet practice, though not explicitly drawn, implied by block logic)\n",
    "        x1_fused = F.relu(x1_fused)\n",
    "\n",
    "        # Stage 2\n",
    "        x2_main = self.enc2_main(x1_fused)\n",
    "        x2_pool = self.pool2(x2_main)\n",
    "        x2_aux = self.enc2_aux(x1_fused)\n",
    "        x2_fused = x2_pool + x2_aux\n",
    "        x2_fused = F.relu(x2_fused)\n",
    "\n",
    "        # Stage 3\n",
    "        x3_main = self.enc3_main(x2_fused)\n",
    "        x3_pool = self.pool3(x3_main)\n",
    "        x3_aux = self.enc3_aux(x2_fused)\n",
    "        x3_fused = x3_pool + x3_aux\n",
    "        x3_fused = F.relu(x3_fused)\n",
    "\n",
    "        # Stage 4\n",
    "        x4_main = self.enc4_main(x3_fused)\n",
    "        x4_pool = self.pool4(x4_main)\n",
    "        x4_aux = self.enc4_aux(x3_fused)\n",
    "        x4_fused = x4_pool + x4_aux\n",
    "        x4_fused = F.relu(x4_fused)\n",
    "\n",
    "        # Bridge\n",
    "        x_bridge = self.bridge(x4_fused)\n",
    "\n",
    "        # --- RFB SKIP GENERATION ---\n",
    "        # The skips come from the fused encoder features before they went to the next level\n",
    "        # Note: The diagram arrows for skips originate from the output of the \"Main\" double conv \n",
    "        # BEFORE pooling/fusion? Or from the fused result?\n",
    "        # Looking at Fig 2: The arrows go Inputs -> Conv -> [Conv] -> right arrow to RFB.\n",
    "        # This implies the skip comes from the STANDARD encoder path before pooling.\n",
    "        \n",
    "        s1 = self.rfb1(x1_main)\n",
    "        s2 = self.rfb2(x2_main)\n",
    "        s3 = self.rfb3(x3_main)\n",
    "        s4 = self.rfb4(x4_main)\n",
    "\n",
    "        # --- DECODER ---\n",
    "        \n",
    "        # Block 4\n",
    "        d4 = self.up4(x_bridge)\n",
    "        d4 = self.up_conv4(d4)\n",
    "        # Concatenate with RFB Skip 4\n",
    "        d4 = torch.cat([d4, s4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        # Block 3\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = self.up_conv3(d3)\n",
    "        d3 = torch.cat([d3, s3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        # Block 2\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = self.up_conv2(d2)\n",
    "        d2 = torch.cat([d2, s2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        # Block 1\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = self.up_conv1(d1)\n",
    "        d1 = torch.cat([d1, s1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        return self.final_conv(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d98885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([1, 3, 256, 256])\n",
      "Output Shape: torch.Size([1, 1, 256, 256])\n",
      "Total Parameters: 36,988,417\n"
     ]
    }
   ],
   "source": [
    "model = CSE_UNet(in_channels=3, num_classes=1)\n",
    "dummy_input = torch.randn(1, 3, 256, 256)\n",
    "output = model(dummy_input)\n",
    "print(f\"Input Shape: {dummy_input.shape}\")\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "\n",
    "# Calculate params\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
